# Gokul Nandakumar

Software, Data, and Cloud Engineer | ETL/ELT Pipelines | Distributed Systems | Cloud Data Platforms

[Portfolio](https://gokuldata.vercel.app/) · [GitHub](https://github.com/Goku007007) · [LinkedIn](https://www.linkedin.com/in/gokul-nandakumar) · [Email](mailto:goku.careers@gmail.com)

---

## About

I am a **Software, Data, and Cloud Engineer** with experience building:

* **Production-grade ETL/ELT pipelines** using Airflow, dbt, Spark, and cloud orchestration.
* **Scalable backend services** (FastAPI, Python) powering analytics and reporting for thousands of users.
* **Cloud-native data platforms** across AWS (S3, Lambda, Glue, EKS), Azure (ADLS, Synapse), Snowflake, Databricks, and BigQuery.

Across roles in **Data Engineering, Analytics Engineering, BI Engineering, Software Engineering, and Cloud Engineering**, I’ve delivered systems that:

* Reduce data preparation and refresh latency by **66–87%**.
* Unify **5TB+** of fragmented civic, SaaS, and marketing datasets into reliable lakehouse architectures.
* Standardize KPI layers, eliminating manual reconciliation and improving executive decision-making.
* Improve query performance by **30–35%** and reduce compute/storage costs by **10–15%**.
* Automate finance, CRM, and marketing workflows, saving **20+ hours/week** of manual effort.

**Technical Stack**
Python · SQL · Airflow · dbt · Spark (PySpark) · Kafka · FastAPI · Snowflake · Redshift · Databricks · BigQuery · Delta Lake · Terraform · Docker · Kubernetes · GitHub Actions · AWS · Azure · Tableau · Power BI

**What I Focus On**

* Dimensional modeling & clean semantic layers
* Cost-efficient cloud architecture and observability
* Reproducible, testable, CI/CD-driven pipelines
* High-performance SQL & distributed data workflows

I am targeting **Software, Data, and Cloud Engineering** roles (US Remote/Hybrid; open to relocate).

---

## Core Skills

**Languages and Databases**  
Python, SQL, JavaScript, Shell Scripting, PHP (Laravel), PostgreSQL, MySQL, Cassandra, DynamoDB, Redis

**Data Engineering and Analytics**  
Airflow, dbt, Spark (PySpark), Kafka, Snowflake, Databricks, Delta Lake, BigQuery, DuckDB, dimensional modeling, Medallion architecture

**Cloud and Infrastructure**  
AWS (S3, Lambda, Glue), Azure (ADLS Gen2, Synapse), Terraform, Docker, Kubernetes, GitHub Actions, CI/CD

**Backend and APIs**  
FastAPI, REST APIs, event-driven services, observability and logging, performance optimization

**AI Tools**  
Cursor, Claude Code, ChatGPT CLI

**Visualization and Reporting**  
Tableau, Power BI, dashboard design, KPI definition and standardization

---

## Highlights

- Reduced weekly civic, student, and wellness data preparation time from 6 hours to 2 hours by engineering automated ETL pipelines in Airflow with dbt models on Snowflake.
- Unified more than 5 TB of open data from BigQuery and Snowflake into a single lakehouse, enabling real-time cross-domain analytics for transit and city operations.
- Reduced CRM data latency from 2 hours to 15 minutes for over 10,000 users using PySpark pipelines orchestrated with Airflow.
- Improved query performance by 35 percent and reduced compute costs by 15 percent on Redshift and Databricks via SQL and cluster tuning.
- Standardized executive reporting by defining core KPIs in Snowflake and surfacing them through Tableau and Power BI.

---

## Selected Projects

### National EV Charging Infrastructure Analytics
A data engineering and analytics project focused on public EV charging infrastructure across the United States.

- Ingested more than 80,000 charging station records into cloud storage and BigQuery, separating raw and curated layers.
- Modeled coverage, uptime, and port availability to evaluate network reliability by state and network provider.
- Built an interactive Tableau dashboard that surfaces coverage gaps, network leaderboards, and expansion opportunities.

### GPU Cluster Telemetry and Observability System
An observability lakehouse for GPU cluster telemetry and cost insights.

- Ingested over 10 million trace rows into a Parquet and DuckDB backend following a Bronze, Silver, and Gold layering strategy.
- Standardized raw JSON logs into queryable tables for utilization, saturation, and error analysis.
- Enabled low-latency queries and daily cost analysis to understand GPU usage patterns and anomalies.

### Additional Work
I also build smaller tools and experiments around:

- Data quality monitoring and alerting  
- Self service analytics layers for non-engineering stakeholders  
- Simple web services and internal tools using FastAPI and modern front-end frameworks  

You can find more in the pinned repositories on this profile.

---

## How I Work

- Design from the warehouse and serving layer backwards, so that models and pipelines are aligned with actual decision-making needs.
- Use AI tools for exploration, scaffolding, and large codebase edits, but keep tests, observability, and production reviews as non-negotiable safeguards.
- Prefer simple, well-documented solutions over clever ones that are hard to maintain.
- Track cost and performance from the beginning instead of treating them as afterthoughts.

---

## Contact

If you would like to talk about data engineering, software systems, or potential opportunities:

- Email: goku.careers@gmail.com  
- Portfolio: https://gokuldata.vercel.app/  
- LinkedIn: https://www.linkedin.com/in/gokul-nandakumar  

Feel free to open an issue or discussion on any repository if you have questions about the implementation details.
