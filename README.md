# Gokul Nandakumar

Software and Data Engineer | ETL/ELT Pipelines | Cloud Data Platforms

[Portfolio](https://gokuldata.vercel.app/) Â· [GitHub](https://github.com/Goku007007) Â· [LinkedIn](https://www.linkedin.com/in/gokul-nandakumar) Â· [Email](mailto:goku.careers@gmail.com)

---

## About

Data Analytics Engineer targeting Data Engineer / Analytics Engineer / BI Engineer / Data Analyst roles in the US (Remote/Hybrid; open to relocate).

**Core stack:** SQL Â· Python Â· dbt Â· Airflow Â· Snowflake/Redshift Â· Databricks Â· Power BI Â· Tableau Â· AWS Â· Azure Â· APIs Â· Git Â· CI/CD.

**Impact:** Designed event-driven ETL/ELT pipelines and standardized models that cut refresh times by **85%+** and power leadership decisions for public-sector and SaaS teams.  
I like **dimensional modeling**, **cost-aware design**, and **crisp storytelling** with data.

ðŸ“© goku.careers@gmail.com â€¢ Chicago, IL â€¢ US Remote/Hybrid â€¢ Immediate start

[![AWS SAA](https://img.shields.io/badge/AWS_Solutions_Architect_Associate-Verified-orange?logo=amazon-aws&logoColor=white)](https://www.credly.com/badges/ca7ad75a-68b9-43d7-9c45-86e757948526)
[![Databricks Fundamentals](https://img.shields.io/badge/Databricks_Fundamentals-Verified-red?logo=databricks&logoColor=white)](https://credentials.databricks.com/ef9e6ee0-6039-4d76-a48e-5e20b258de52#acc.5CGS2qXj)

---

## Core Skills

**Languages and Databases**  
Python, SQL, JavaScript, Shell Scripting, PHP (Laravel), PostgreSQL, MySQL, Cassandra, DynamoDB, Redis

**Data Engineering and Analytics**  
Airflow, dbt, Spark (PySpark), Kafka, Snowflake, Databricks, Delta Lake, BigQuery, DuckDB, dimensional modeling, Medallion architecture

**Cloud and Infrastructure**  
AWS (S3, Lambda, Glue), Azure (ADLS Gen2, Synapse), Terraform, Docker, Kubernetes, GitHub Actions, CI/CD

**Backend and APIs**  
FastAPI, REST APIs, event-driven services, observability and logging, performance optimization

**AI Tools**  
Cursor, Claude Code, ChatGPT CLI

**Visualization and Reporting**  
Tableau, Power BI, dashboard design, KPI definition and standardization

---

## Highlights

- Reduced weekly civic, student, and wellness data preparation time from 6 hours to 2 hours by engineering automated ETL pipelines in Airflow with dbt models on Snowflake.
- Unified more than 5 TB of open data from BigQuery and Snowflake into a single lakehouse, enabling real-time cross-domain analytics for transit and city operations.
- Reduced CRM data latency from 2 hours to 15 minutes for over 10,000 users using PySpark pipelines orchestrated with Airflow.
- Improved query performance by 35 percent and reduced compute costs by 15 percent on Redshift and Databricks via SQL and cluster tuning.
- Standardized executive reporting by defining core KPIs in Snowflake and surfacing them through Tableau and Power BI.

---

## Selected Projects

### National EV Charging Infrastructure Analytics
A data engineering and analytics project focused on public EV charging infrastructure across the United States.

- Ingested more than 80,000 charging station records into cloud storage and BigQuery, separating raw and curated layers.
- Modeled coverage, uptime, and port availability to evaluate network reliability by state and network provider.
- Built an interactive Tableau dashboard that surfaces coverage gaps, network leaderboards, and expansion opportunities.

### GPU Cluster Telemetry and Observability System
An observability lakehouse for GPU cluster telemetry and cost insights.

- Ingested over 10 million trace rows into a Parquet and DuckDB backend following a Bronze, Silver, and Gold layering strategy.
- Standardized raw JSON logs into queryable tables for utilization, saturation, and error analysis.
- Enabled low-latency queries and daily cost analysis to understand GPU usage patterns and anomalies.

### Additional Work
I also build smaller tools and experiments around:

- Data quality monitoring and alerting  
- Self service analytics layers for non-engineering stakeholders  
- Simple web services and internal tools using FastAPI and modern front-end frameworks  

You can find more in the pinned repositories on this profile.

---

## How I Work

- Design from the warehouse and serving layer backwards, so that models and pipelines are aligned with actual decision-making needs.
- Use AI tools for exploration, scaffolding, and large codebase edits, but keep tests, observability, and production reviews as non-negotiable safeguards.
- Prefer simple, well-documented solutions over clever ones that are hard to maintain.
- Track cost and performance from the beginning instead of treating them as afterthoughts.

---

## Contact

If you would like to talk about data engineering, software systems, or potential opportunities:

- Email: goku.careers@gmail.com  
- Portfolio: https://gokuldata.vercel.app/  
- LinkedIn: https://www.linkedin.com/in/gokul-nandakumar  

Feel free to open an issue or discussion on any repository if you have questions about the implementation details.
